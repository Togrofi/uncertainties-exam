{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Togrofi/uncertainties-exam/blob/main/UncertaintiesExam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9oM3NP03lt0h"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.nn import Parameter\n",
        "from torch.utils.data import Subset, DataLoader, RandomSampler, SubsetRandomSampler\n",
        "from torch.optim import Adam, SGD\n",
        "from tqdm import tqdm\n",
        "from hyperopt import hp, tpe, fmin\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "params = {\n",
        "    'beta': 1,\n",
        "    'proportion_of_dataset': 1,\n",
        "    'threshold': 0.1,\n",
        "    'experiment_name': 'split', # 'split', 'permuted', 'random'\n",
        "    'perform_vcl': True,\n",
        "    'perform_auto_vcl': True,\n",
        "    'batch_size': 256,\n",
        "}\n",
        "\n",
        "\n",
        "permuted_params = {\n",
        "    'epochs': 10,\n",
        "    'coreset_size': 200,\n",
        "    'num_tasks': 10,\n",
        "    'hidden_dim': 100\n",
        "}\n",
        "\n",
        "split_params = {\n",
        "    'epochs': 10,\n",
        "    'coreset_size': 40,\n",
        "    'hidden_dim': 256\n",
        "}\n",
        "\n",
        "random_split_params = {\n",
        "    'epochs': 10,\n",
        "    'coreset_size': 40,\n",
        "    'num_clients': 10,\n",
        "    'hidden_dim': 256\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6T3YPtufAir"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ynx9u5CqNRGA"
      },
      "outputs": [],
      "source": [
        "#@title Create Permuted Data\n",
        "def get_permuted_dataloaders(num_tasks):\n",
        "    batch_size = params['batch_size']\n",
        "    dataset = datasets.MNIST\n",
        "    dataloaders = []\n",
        "\n",
        "    for task in range(num_tasks):\n",
        "        if task == 0:\n",
        "            transform = transforms.Compose([\n",
        "                transforms.Resize((28, 28)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.1307,), (0.3081,))\n",
        "            ])\n",
        "        else:\n",
        "            rng_permute = np.random.RandomState(task)\n",
        "            idx_permute = torch.from_numpy(rng_permute.permutation(28*28))\n",
        "            transform = transforms.Compose([\n",
        "                transforms.Resize((28, 28)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.1307,), (0.3081,)),\n",
        "                transforms.Lambda(lambda x, idx=idx_permute: x.view(-1)[idx].view(1, 28, 28))\n",
        "            ])\n",
        "\n",
        "        full_task_trainset = dataset(root=\"./data\", train=True, download=True, transform=transform)\n",
        "        jump_size = int(1 / params['proportion_of_dataset'])\n",
        "        task_trainset = Subset(full_task_trainset, range(0, len(full_task_trainset), jump_size))\n",
        "        task_train_sampler = RandomSampler(task_trainset)\n",
        "        task_train_loader = DataLoader(task_trainset, batch_size=batch_size, sampler=task_train_sampler)\n",
        "\n",
        "        full_task_testset = dataset(root=\"./data\", train=False, download=True, transform=transform)\n",
        "        jump_size = int(1 / params['proportion_of_dataset'])\n",
        "        task_testset = Subset(full_task_testset, range(0, len(full_task_testset), jump_size))\n",
        "        task_test_sampler = RandomSampler(task_testset)\n",
        "        task_test_loader = DataLoader(task_testset, batch_size=batch_size, sampler=task_test_sampler)\n",
        "\n",
        "        dataloaders.append((task_train_loader, task_test_loader))\n",
        "\n",
        "    return dataloaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eHkWKwgMJyzl"
      },
      "outputs": [],
      "source": [
        "#@title Create Split Data\n",
        "def _extract_class_specific_idx(dataset, target_classes):\n",
        "    idx = torch.zeros_like(dataset.targets, dtype=torch.bool)\n",
        "    for target in target_classes:\n",
        "        idx = idx | (dataset.targets==target)\n",
        "\n",
        "    return idx\n",
        "\n",
        "def get_split_dataloaders(class_distribution):\n",
        "    batch_size = params['batch_size']\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((28, 28)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    dataset = datasets.MNIST\n",
        "    trainset = dataset(root=\"./data\", train=True, download=True, transform=transform)\n",
        "    testset = dataset(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "    dataloaders = []\n",
        "\n",
        "    for i, classes in enumerate(class_distribution):\n",
        "        offset = 2 * i\n",
        "        train_idx = _extract_class_specific_idx(trainset, classes)\n",
        "        train_idx = torch.where(train_idx)[0]\n",
        "        sub_train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
        "        sub_train_loader = torch.utils.data.DataLoader(\n",
        "            trainset, batch_size=batch_size, sampler=sub_train_sampler)\n",
        "        trainset.targets[sub_train_sampler.indices] -= offset\n",
        "\n",
        "        test_idx = _extract_class_specific_idx(testset, classes)\n",
        "        test_idx = torch.where(test_idx)[0]\n",
        "        sub_test_sampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
        "        sub_test_loader = torch.utils.data.DataLoader(\n",
        "            testset, batch_size=batch_size, sampler=sub_test_sampler)\n",
        "        testset.targets[sub_test_sampler.indices] -= offset\n",
        "\n",
        "        dataloaders.append((sub_train_loader, sub_test_loader))\n",
        "\n",
        "    random.shuffle(dataloaders)\n",
        "    return dataloaders\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jvLvzdhf1aiM"
      },
      "outputs": [],
      "source": [
        "#@title Create Random Split Data\n",
        "def get_random_split_dataloaders(class_distribution, num_clients):\n",
        "    batch_size = params['batch_size']\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((28, 28)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    dataset = datasets.MNIST\n",
        "    trainset = dataset(root=\"./data\", train=True, download=True, transform=transform)\n",
        "    testset = dataset(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "    all_tasks = []\n",
        "\n",
        "    for i, classes in enumerate(class_distribution):\n",
        "        offset = 2 * i\n",
        "        train_idx = _extract_class_specific_idx(trainset, classes)\n",
        "        train_idx = torch.where(train_idx)[0]\n",
        "        sub_train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
        "        sub_train_loader = torch.utils.data.DataLoader(\n",
        "            trainset, batch_size=batch_size, sampler=sub_train_sampler)\n",
        "        trainset.targets[sub_train_sampler.indices] -= offset\n",
        "\n",
        "        test_idx = _extract_class_specific_idx(testset, classes)\n",
        "        test_idx = torch.where(test_idx)[0]\n",
        "        sub_test_sampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
        "        sub_test_loader = torch.utils.data.DataLoader(\n",
        "            testset, batch_size=batch_size, sampler=sub_test_sampler)\n",
        "        testset.targets[sub_test_sampler.indices] -= offset\n",
        "\n",
        "        all_tasks.append((sub_train_loader, sub_test_loader))\n",
        "\n",
        "    dataloaders = []\n",
        "    for _ in range(num_clients):\n",
        "        i = random.choice(list(range(len(all_tasks))))\n",
        "        print(i)\n",
        "        dataloaders.append(all_tasks[i])\n",
        "\n",
        "    return dataloaders\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQgAeOrKfF6R"
      },
      "source": [
        "# Custom Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BOQxQIgJdpos"
      },
      "outputs": [],
      "source": [
        "#@title Custom Layer\n",
        "def KL_DIV(mu_p, sig_p, mu_q, sig_q):\n",
        "    kl = 0.5 * (2 * torch.log(sig_p / sig_q) - 1 + (sig_q / sig_p).pow(2) + ((mu_p - mu_q) / sig_p).pow(2)).sum()\n",
        "    return kl\n",
        "\n",
        "class BayesLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, bias=True, priors=None):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.use_bias = bias\n",
        "\n",
        "        if priors is None:\n",
        "            priors = {\n",
        "                'prior_mu': 0,\n",
        "                'prior_sigma': 0.01,\n",
        "            }\n",
        "\n",
        "        self.prior_W_mu = torch.tensor(priors['prior_mu'])\n",
        "        self.prior_W_sigma = torch.tensor(priors['prior_sigma'])\n",
        "        self.prior_bias_mu = torch.tensor(priors['prior_mu'])\n",
        "        self.prior_bias_sigma = torch.tensor(priors['prior_sigma'])\n",
        "\n",
        "        self.W_mu = Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.W_rho = Parameter(torch.Tensor(out_features, in_features))\n",
        "        if self.use_bias:\n",
        "            self.bias_mu = Parameter(torch.Tensor(out_features))\n",
        "            self.bias_rho = Parameter(torch.Tensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias_mu', None)\n",
        "            self.register_parameter('bias_rho', None)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.W_mu.data.normal_(0, 0.1)\n",
        "        self.W_rho.data.fill_(-3)\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias_mu.data.normal_(0, 0.1)\n",
        "            self.bias_rho.data.fill_(-3)\n",
        "\n",
        "    def forward(self, x, sample=True):\n",
        "        self.W_sigma = torch.log1p(torch.exp(self.W_rho))\n",
        "        if self.use_bias:\n",
        "            self.bias_sigma = torch.log1p(torch.exp(self.bias_rho))\n",
        "            bias_var = self.bias_sigma ** 2\n",
        "        else:\n",
        "            self.bias_sigma = bias_var = None\n",
        "\n",
        "        act_mu = F.linear(x, self.W_mu, self.bias_mu)\n",
        "        # What is this line?\n",
        "        act_var = 1e-16 + F.linear(x ** 2, self.W_sigma ** 2, bias_var)\n",
        "        act_std = torch.sqrt(act_var)\n",
        "\n",
        "        if self.training or sample:\n",
        "            eps = torch.empty(act_mu.size()).normal_(0, 1).to(act_mu.device)\n",
        "            return act_mu + act_std * eps\n",
        "        else:\n",
        "            return act_mu\n",
        "\n",
        "    def kl_loss(self):\n",
        "        kl = KL_DIV(self.prior_W_mu, self.prior_W_sigma, self.W_mu, self.W_sigma)\n",
        "        if self.use_bias:\n",
        "            kl += KL_DIV(self.prior_bias_mu, self.prior_bias_sigma, self.bias_mu, self.bias_sigma)\n",
        "        return kl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "X_09osLpRh7O"
      },
      "outputs": [],
      "source": [
        "#@title Dynamic Head Model\n",
        "\n",
        "class DynamicHeadModel(nn.Module):\n",
        "    def __init__(self, input_size=28*28, hidden_dim=100, hidden_layers=2):\n",
        "        super().__init__()\n",
        "        self.input_layer = BayesLinear(input_size, hidden_dim)\n",
        "        self.shared_layers = nn.ModuleList([BayesLinear(hidden_dim, hidden_dim) for _ in range(hidden_layers)])\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Initially empty, will create new heads dynamically as batches come in.\n",
        "        self.head_layers = nn.ModuleList()\n",
        "        self.num_heads = 0\n",
        "\n",
        "        self.likelihoods = []\n",
        "\n",
        "        # Create a lookup dictionary so that each batch index is linked to the\n",
        "        # index of the head that gets used on that batch (unlike in the VCL paper\n",
        "        # there is not necessarily one head per batch).\n",
        "        self.client_id_to_head_id = {}\n",
        "\n",
        "    def forward(self, x, id, use_client_id=False):\n",
        "        head_id = self.client_id_to_head_id[id] if use_client_id else id\n",
        "\n",
        "        x = x.view(-1, 784)\n",
        "        x = F.relu(self.input_layer(x))\n",
        "\n",
        "        for layer in self.shared_layers:\n",
        "            x = F.relu(layer(x))\n",
        "\n",
        "        head_to_use = self.head_layers[head_id]\n",
        "\n",
        "        return head_to_use(x)\n",
        "\n",
        "    def create_new_head(self, client_id, width):\n",
        "        print('\\nCreating a new head!')\n",
        "        new_head_id = len(self.head_layers) # 0 if no head layers yet, 1 if there is 1 etc\n",
        "        self.client_id_to_head_id[client_id] = new_head_id\n",
        "        self.head_layers.append(BayesLinear(self.hidden_dim, width).to(device))\n",
        "        self.num_heads += 1\n",
        "\n",
        "    def get_kl(self, id, use_client_id=False):\n",
        "        head_id = self.client_id_to_head_id[id] if use_client_id else id\n",
        "        kl = 0.0\n",
        "        kl += self.input_layer.kl_loss()\n",
        "        for layer in self.shared_layers:\n",
        "            kl += layer.kl_loss()\n",
        "\n",
        "        kl += self.head_layers[head_id].kl_loss()\n",
        "\n",
        "        return kl\n",
        "\n",
        "    def update_prior(self):\n",
        "        for child in self.children():\n",
        "            if child.__class__ is not nn.ModuleList:\n",
        "                child.prior_W_mu = child.W_mu.data\n",
        "                child.prior_W_sigma = child.W_sigma.data\n",
        "                if child.use_bias:\n",
        "                    child.prior_bias_mu = child.bias_mu.data\n",
        "                    child.prior_bias_sigma = child.bias_sigma.data\n",
        "            if child.__class__ is nn.ModuleList:\n",
        "                for layer in child:\n",
        "                    layer.prior_W_mu = layer.W_mu.data\n",
        "                    layer.prior_W_sigma = layer.W_sigma.data\n",
        "                    if layer.use_bias:\n",
        "                        layer.prior_bias_mu = layer.bias_mu.data\n",
        "                        layer.prior_bias_sigma = layer.bias_sigma.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6aiCCIUYmk8"
      },
      "source": [
        "# Baseline Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xTK2oDN0YdCX"
      },
      "outputs": [],
      "source": [
        "#@title Baseline Permuted Model\n",
        "class PermutedModel(nn.Module):\n",
        "    def __init__(self, input_size=28*28, hidden_dim=100, output_dim=10):\n",
        "        self.input_size = input_size\n",
        "        super().__init__()\n",
        "        self.fc1 = BayesLinear(input_size, hidden_dim)\n",
        "        self.fc2 = BayesLinear(hidden_dim, hidden_dim)\n",
        "        self.head_layer = BayesLinear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, task_id, use_client_id=False):\n",
        "        out = x.view(-1, self.input_size)\n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = F.relu(self.fc2(out))\n",
        "        return self.head_layer(out)\n",
        "\n",
        "    def get_kl(self, task_id):\n",
        "        kl = 0.0\n",
        "        kl += self.fc1.kl_loss()\n",
        "        kl += self.fc2.kl_loss()\n",
        "        kl += self.head_layer.kl_loss()\n",
        "        return kl\n",
        "\n",
        "    def update_prior(self):\n",
        "        self.fc1.prior_W_mu = self.fc1.W_mu.data\n",
        "        self.fc1.prior_W_sigma = self.fc1.W_sigma.data\n",
        "        if self.fc1.use_bias:\n",
        "            self.fc1.prior_bias_mu = self.fc1.bias_mu.data\n",
        "            self.fc1.prior_bias_sigma = self.fc1.bias_sigma.data\n",
        "\n",
        "        self.fc2.prior_W_mu = self.fc2.W_mu.data\n",
        "        self.fc2.prior_W_sigma = self.fc2.W_sigma.data\n",
        "        if self.fc2.use_bias:\n",
        "            self.fc2.prior_bias_mu = self.fc2.bias_mu.data\n",
        "            self.fc2.prior_bias_sigma = self.fc2.bias_sigma.data\n",
        "\n",
        "        self.head_layer.prior_W_mu = self.head_layer.W_mu.data\n",
        "        self.head_layer.prior_W_sigma = self.head_layer.W_sigma.data\n",
        "        if self.head_layer.use_bias:\n",
        "            self.head_layer.prior_bias_mu = self.head_layer.bias_mu.data\n",
        "            self.head_layer.prior_bias_sigma = self.head_layer.bias_sigma.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_-C0C4H6YiDW"
      },
      "outputs": [],
      "source": [
        "#@title Baseline Split Model\n",
        "class SplitModel(nn.Module):\n",
        "    def __init__(self, input_size=28*28, hidden_dim=100, output_dim=2):\n",
        "        self.input_size = input_size\n",
        "        super().__init__()\n",
        "        self.fc1 = BayesLinear(input_size, hidden_dim)\n",
        "        self.fc2 = BayesLinear(hidden_dim, hidden_dim)\n",
        "        self.head_layers = nn.ModuleList([BayesLinear(hidden_dim, output_dim) for i in range(5)])\n",
        "\n",
        "    def forward(self, x, task_id, use_client_id=False):\n",
        "        out = x.view(-1, self.input_size)\n",
        "        for layer in self.children():\n",
        "            if layer.__class__ is not nn.ModuleList:\n",
        "                out = F.relu(layer(out))\n",
        "        return self.head_layers[task_id](out)\n",
        "\n",
        "    def get_kl(self, task_id):\n",
        "        kl = 0.0\n",
        "        for layer in self.children():\n",
        "            if layer.__class__ is not nn.ModuleList:\n",
        "                kl += layer.kl_loss()\n",
        "        kl += self.head_layers[task_id].kl_loss()\n",
        "        return kl\n",
        "\n",
        "    def update_prior(self):\n",
        "        for layer in self.children():\n",
        "            if layer.__class__ is not nn.ModuleList:\n",
        "                layer.prior_W_mu = layer.W_mu.data\n",
        "                layer.prior_W_sigma = layer.W_sigma.data\n",
        "                if layer.use_bias:\n",
        "                    layer.prior_bias_mu = layer.bias_mu.data\n",
        "                    layer.prior_bias_sigma = layer.bias_sigma.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8IlTCRLfT4b"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_12aDBaPiOUl"
      },
      "outputs": [],
      "source": [
        "#@title Coreset\n",
        "def attach_random_coreset_permuted(coresets, sub_train_loader, num_samples=200):\n",
        "    shuffled_task_indices = torch.randperm(len(sub_train_loader.dataset))\n",
        "    coreset_indices = shuffled_task_indices[:num_samples]\n",
        "    coreset_sampler = SubsetRandomSampler(coreset_indices)\n",
        "    coreset_loader = DataLoader(\n",
        "        sub_train_loader.dataset, batch_size=sub_train_loader.batch_size, sampler=coreset_sampler\n",
        "    )\n",
        "    coresets.append(coreset_loader)\n",
        "\n",
        "def attach_random_coreset_split(coresets, sub_train_loader, num_samples=200):\n",
        "    task_indices = sub_train_loader.sampler.indices\n",
        "    shuffled_task_indices = task_indices[torch.randperm(len(task_indices))]\n",
        "    coreset_indices = shuffled_task_indices[:num_samples]\n",
        "    sub_train_loader.sampler.indices = shuffled_task_indices[num_samples:]  # Delete coreset from orginal data\n",
        "    coreset_sampler = torch.utils.data.SubsetRandomSampler(coreset_indices)\n",
        "    coreset_loader = torch.utils.data.DataLoader(\n",
        "        sub_train_loader.dataset, batch_size=sub_train_loader.batch_size, sampler=coreset_sampler)\n",
        "    coresets.append(coreset_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rDy55WNx61Qc"
      },
      "outputs": [],
      "source": [
        "#@title ELBO\n",
        "class ELBO(nn.Module):\n",
        "    def __init__(self, model, beta):\n",
        "        super().__init__()\n",
        "        self.num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "        self.beta = beta\n",
        "\n",
        "    def forward(self, outputs, targets, kl):\n",
        "        assert not targets.requires_grad\n",
        "        return F.nll_loss(outputs, targets, reduction='mean') + self.beta * kl / self.num_params\n",
        "\n",
        "def calculate_accuracy(outputs, targets):\n",
        "    return np.mean(outputs.argmax(dim=-1).cpu().numpy() == targets.cpu().numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "thJ8BgM2EMH7"
      },
      "outputs": [],
      "source": [
        "#@title Get Probability of Head Being the Correct One\n",
        "def get_probability_of_head(model, inputs, targets, output_nodes, head_id):\n",
        "    log_output = monte_carlo(model, inputs, output_nodes, head_id)\n",
        "    loss = F.nll_loss(log_output, targets, reduction='mean')\n",
        "    probability = torch.exp(-loss).item()\n",
        "    return probability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VWCYS5W1kcM0"
      },
      "outputs": [],
      "source": [
        "#@title Monte Carlo\n",
        "def monte_carlo(model, inputs, output_nodes, id, use_client_id=False, no_grad=False):\n",
        "    T = 10\n",
        "    outputs = torch.zeros(inputs.shape[0], output_nodes, T).to(device)\n",
        "\n",
        "    for i in range(T):\n",
        "        if no_grad:\n",
        "            with torch.no_grad():\n",
        "                out = model(inputs, id, use_client_id)\n",
        "        else:\n",
        "            out = model(inputs, id, use_client_id)\n",
        "        outputs[:, :, i] = F.log_softmax(out, dim=-1)\n",
        "\n",
        "    log_output = torch.logsumexp(outputs, dim=-1) - np.log(T)\n",
        "\n",
        "    return log_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cILWlU6VYz96"
      },
      "source": [
        "# VCL Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BhNa39IfixRs"
      },
      "outputs": [],
      "source": [
        "#@title Auto VCL Train and Predict\n",
        "def dynamic_train(model, num_epochs, dataloader, client_id, beta, replay=False):\n",
        "    beta = 0 if replay else beta\n",
        "    lr_start = 1e-3\n",
        "\n",
        "    flattened_tensor = torch.cat([t for _, t in dataloader])\n",
        "    output_nodes = torch.max(flattened_tensor).item() + 1\n",
        "\n",
        "    elbo = ELBO(model, beta)\n",
        "    optimizer = Adam(model.parameters(), lr=lr_start)\n",
        "\n",
        "    head_loss = None\n",
        "\n",
        "    model.train()\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "    #for epoch in range(num_epochs):\n",
        "        for inputs, targets in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            # Check if we have identified this client_id with a certain head already:\n",
        "            if client_id not in model.client_id_to_head_id.keys():\n",
        "                # Otherwise, we need to find which head to use.\n",
        "                # First, calculate the loss for each existing head:\n",
        "                most_promising_head_id   = None\n",
        "                most_promising_head_prob = 0.\n",
        "                for head_id, head in enumerate(model.head_layers):\n",
        "                    if head.out_features == output_nodes:\n",
        "                        prob_of_this_head = get_probability_of_head(model, inputs, targets, output_nodes, head_id)\n",
        "                        if prob_of_this_head > most_promising_head_prob:\n",
        "                            most_promising_head_id   = head_id\n",
        "                            most_promising_head_prob = prob_of_this_head\n",
        "\n",
        "                print('\\nBest head likelihood: ', most_promising_head_prob)\n",
        "                model.likelihoods.append(most_promising_head_prob)\n",
        "\n",
        "                # If the results are too poor, create a new head for it\n",
        "                # and calculate the loss on this new head.\n",
        "                if most_promising_head_prob <= params['threshold']:\n",
        "                    # We couldn't find a strong head, so make a new one.\n",
        "                    model.create_new_head(client_id, output_nodes)\n",
        "                else:\n",
        "                    model.client_id_to_head_id[client_id] = most_promising_head_id\n",
        "\n",
        "            log_output = monte_carlo(model, inputs, output_nodes, client_id, use_client_id=True)\n",
        "            kl = model.get_kl(client_id, use_client_id=True)\n",
        "            head_loss = elbo(log_output, targets, kl)\n",
        "            head_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "def dynamic_predict(model, dataloader, client_id):\n",
        "    model.eval()\n",
        "    accs = []\n",
        "\n",
        "    flattened_tensor = torch.cat([t.flatten() for _, t in dataloader])\n",
        "    output_nodes = torch.max(flattened_tensor).item() + 1\n",
        "\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        log_output = monte_carlo(model, inputs, output_nodes, client_id, use_client_id=True, no_grad=True)\n",
        "        accs.append(calculate_accuracy(log_output, targets))\n",
        "\n",
        "    return np.mean(accs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Sy3T_IQFVSzT"
      },
      "outputs": [],
      "source": [
        "#@title Auto VCL\n",
        "def dynamic_vcl(num_epochs, dataloaders, model, coreset_method, coreset_size=0, beta=1.):\n",
        "    num_batches = len(dataloaders)\n",
        "    coreset_list = []\n",
        "    all_accs = np.empty(shape=(num_batches, num_batches))\n",
        "    all_accs.fill(np.nan)\n",
        "\n",
        "    for client_id, (trainloader, testloader) in enumerate(dataloaders):\n",
        "        # Train on non-coreset data\n",
        "        dynamic_train(model, num_epochs, trainloader, client_id, beta)\n",
        "        print(\"Done Training Task\", client_id + 1)\n",
        "\n",
        "        # Attach a new coreset\n",
        "        if coreset_size > 0:\n",
        "            coreset_method(coreset_list, trainloader, num_samples=coreset_size)\n",
        "\n",
        "            # Replay old tasks using coresets\n",
        "            for task in range(client_id + 1):\n",
        "                print(\"Replaying Task\", task + 1)\n",
        "                dynamic_train(model, num_epochs, coreset_list[task], task, beta, replay=True)\n",
        "        print()\n",
        "\n",
        "        # Evaluate on old tasks\n",
        "        for client_to_retest_id in range(client_id + 1):\n",
        "            _, testloader_i = dataloaders[client_to_retest_id]\n",
        "            accuracy = dynamic_predict(model, testloader_i, client_to_retest_id)\n",
        "            print(\"Task {} Accuracy: {}\".format(client_to_retest_id + 1, accuracy))\n",
        "            all_accs[client_id][client_to_retest_id] = accuracy\n",
        "        print()\n",
        "\n",
        "        model.update_prior()\n",
        "    print(all_accs)\n",
        "    return all_accs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ahYdBEamfdc4"
      },
      "outputs": [],
      "source": [
        "#@title Baseline Train and Predict\n",
        "def baseline_train(model, num_epochs, dataloader, task_id, replay=False):\n",
        "    beta = 0 if replay else 1\n",
        "    lr_start = 1e-3\n",
        "\n",
        "    flattened_tensor = torch.cat([t.flatten() for _, t in dataloader])\n",
        "    output_nodes = torch.max(flattened_tensor).item() + 1\n",
        "\n",
        "    elbo = ELBO(model, beta)\n",
        "    optimizer = Adam(model.parameters(), lr=lr_start)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "        for inputs, targets in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            log_output = monte_carlo(model, inputs, output_nodes, task_id)\n",
        "\n",
        "            kl = model.get_kl(task_id)\n",
        "            loss = elbo(log_output, targets, kl)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "def baseline_predict(model, dataloader, task_id):\n",
        "    flattened_tensor = torch.cat([t.flatten() for _, t in dataloader])\n",
        "    output_nodes = torch.max(flattened_tensor).item() + 1\n",
        "\n",
        "    model.train()\n",
        "    accs = []\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        log_output = monte_carlo(model, inputs, output_nodes, task_id, no_grad=True)\n",
        "        accs.append(calculate_accuracy(log_output, targets))\n",
        "\n",
        "    return np.mean(accs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "t9uWtW39YTc6"
      },
      "outputs": [],
      "source": [
        "#@title Baseline VCL\n",
        "def baseline_vcl(num_tasks, num_epochs, dataloaders, model, coreset_method, coreset_size=0):\n",
        "    coreset_list = []\n",
        "    all_accs = np.empty(shape=(num_tasks, num_tasks))\n",
        "    all_accs.fill(np.nan)\n",
        "    for task_id in range(num_tasks):\n",
        "        print(\"Starting Task\", task_id + 1)\n",
        "\n",
        "        # Train on non-coreset data\n",
        "        trainloader, testloader = dataloaders[task_id]\n",
        "        baseline_train(model, num_epochs, trainloader, task_id)\n",
        "        print(\"Done Training Task\", task_id + 1)\n",
        "\n",
        "        # Attach a new coreset\n",
        "        if coreset_size > 0:\n",
        "            coreset_method(coreset_list, trainloader, num_samples=coreset_size)\n",
        "\n",
        "            # Replay old tasks using coresets\n",
        "            for task in range(task_id + 1):\n",
        "                print(\"Replaying Task\", task + 1)\n",
        "                baseline_train(model, num_epochs, coreset_list[task], task_id=task, replay=True)\n",
        "        print()\n",
        "\n",
        "        # Evaluate on old tasks\n",
        "        for task in range(task_id + 1):\n",
        "            _, testloader_i = dataloaders[task]\n",
        "            accuracy = baseline_predict(model, testloader_i, task)\n",
        "            print(\"Task {} Accuracy: {}\".format(task + 1, accuracy))\n",
        "            all_accs[task_id][task] = accuracy\n",
        "        print()\n",
        "        #if update_prior:\n",
        "        model.update_prior()\n",
        "    print(all_accs)\n",
        "    return all_accs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4gBsipVfuC1"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 828
        },
        "id": "8sBF5xv2DIpw",
        "outputId": "6eb1e1f9-a54a-4fd0-d856-4ad9d483ebc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 75932795.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 97454299.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 34653268.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 22307410.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best head likelihood:  0.0\n",
            "\n",
            "Creating a new head!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 3/10 [00:28<01:07,  9.58s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-c27664b175e8>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'perform_auto_vcl'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mauto_vcl_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDynamicHeadModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mauto_vcl_accs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdynamic_vcl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_vcl_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattach_random_coreset_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoreset_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'beta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'perform_vcl'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mvcl_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSplitModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-abe18b0104d3>\u001b[0m in \u001b[0;36mdynamic_vcl\u001b[0;34m(num_epochs, dataloaders, model, coreset_method, coreset_size, beta)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mclient_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Train on non-coreset data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mdynamic_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done Training Task\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-287d5b6f2d8b>\u001b[0m in \u001b[0;36mdynamic_train\u001b[0;34m(model, num_epochs, dataloader, client_id, beta, replay)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#for epoch in range(num_epochs):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \"\"\"\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    453\u001b[0m             )\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mget_dimensions\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \"\"\"\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_dimensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/utils.py\u001b[0m in \u001b[0;36m_log_api_usage_once\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFunctionType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{module}.{name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#@title Run Experiments\n",
        "split_class_distribution = [\n",
        "    [0, 1],\n",
        "    [2, 3],\n",
        "    [4, 5],\n",
        "    [6, 7],\n",
        "    [8, 9]]\n",
        "\n",
        "if params['experiment_name'] == 'permuted':\n",
        "    num_tasks = permuted_params['num_tasks']\n",
        "    coreset_size = permuted_params['coreset_size']\n",
        "    permuted_dataloaders = get_permuted_dataloaders(num_tasks)\n",
        "    epochs = permuted_params['epochs']\n",
        "    hidden_dim = permuted_params['hidden_dim']\n",
        "    if params['perform_vcl']:\n",
        "        vcl_model = PermutedModel(hidden_dim=hidden_dim).to(device)\n",
        "        vcl_accs = baseline_vcl(num_tasks, epochs, permuted_dataloaders, vcl_model, attach_random_coreset_permuted, coreset_size)\n",
        "    if params['perform_auto_vcl']:\n",
        "        auto_vcl_model = DynamicHeadModel(hidden_dim=hidden_dim).to(device)\n",
        "        auto_vcl_accs = dynamic_vcl(epochs, permuted_dataloaders, auto_vcl_model, attach_random_coreset_permuted, coreset_size, params['beta'])\n",
        "elif params['experiment_name'] == 'split':\n",
        "    coreset_size = split_params['coreset_size']\n",
        "    split_dataloaders = get_split_dataloaders(split_class_distribution)\n",
        "    epochs = split_params['epochs']\n",
        "    hidden_dim = split_params['hidden_dim']\n",
        "    if params['perform_auto_vcl']:\n",
        "        auto_vcl_model = DynamicHeadModel(hidden_dim=hidden_dim).to(device)\n",
        "        auto_vcl_accs = dynamic_vcl(epochs, split_dataloaders, auto_vcl_model, attach_random_coreset_split, coreset_size, params['beta'])\n",
        "    if params['perform_vcl']:\n",
        "        vcl_model = SplitModel(hidden_dim=hidden_dim).to(device)\n",
        "        vcl_accs = baseline_vcl(5, epochs, split_dataloaders, vcl_model, attach_random_coreset_split, coreset_size)\n",
        "elif params['experiment_name'] == 'random':\n",
        "    coreset_size = random_split_params['coreset_size']\n",
        "    random_split_dataloaders = get_random_split_dataloaders(split_class_distribution, random_split_params['num_clients'])\n",
        "    epochs = random_split_params['epochs']\n",
        "    hidden_dim = random_split_params['hidden_dim']\n",
        "    if params['perform_auto_vcl']:\n",
        "        auto_vcl_model = DynamicHeadModel(hidden_dim=hidden_dim).to(device)\n",
        "        auto_vcl_accs = dynamic_vcl(epochs, random_split_dataloaders, auto_vcl_model, attach_random_coreset_split, coreset_size, params['beta'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFK4agP-r86m"
      },
      "source": [
        "[0,\n",
        "0.0719,\n",
        "0.0217,\n",
        "0.0088,\n",
        "0.0064,\n",
        "0.0157,\n",
        "0.0095,\n",
        "0.0025,\n",
        "0.0075,\n",
        "0.0033]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPQ8lD-7Kd6J"
      },
      "source": [
        "VCL permuted, 200 coreset, 256 width, 10 epochs\n",
        "\n",
        "[[0.90957031        nan        nan        nan        nan        nan\n",
        "         nan        nan        nan        nan]\n",
        " [0.84833984 0.96435547        nan        nan        nan        nan\n",
        "         nan        nan        nan        nan]\n",
        " [0.85703125 0.93925781 0.96386719        nan        nan        nan\n",
        "         nan        nan        nan        nan]\n",
        " [0.80947266 0.93740234 0.94863281 0.96210938        nan        nan\n",
        "         nan        nan        nan        nan]\n",
        " [0.81455078 0.92714844 0.93095703 0.93476563 0.96289062        nan\n",
        "         nan        nan        nan        nan]\n",
        " [0.81083984 0.88427734 0.90664062 0.88740234 0.92841797 0.95898438\n",
        "         nan        nan        nan        nan]\n",
        " [0.77724609 0.88798828 0.90742188 0.88134766 0.90771484 0.93261719\n",
        "  0.95458984        nan        nan        nan]\n",
        " [0.79570312 0.8265625  0.88544922 0.87021484 0.88046875 0.92226562\n",
        "  0.93925781 0.95087891        nan        nan]\n",
        " [0.75341797 0.83710938 0.89394531 0.84003906 0.88671875 0.8984375\n",
        "  0.91806641 0.90839844 0.93154297        nan]\n",
        " [0.73173828 0.84921875 0.84716797 0.84707031 0.87412109 0.89970703\n",
        "  0.88632813 0.90429688 0.90976563 0.95371094]]\n",
        "\n",
        "AutoVCL permuted, 200 coreset, 256 width, 10 epochs, threshold 0.1\n",
        "\n",
        "  [[0.93964844        nan        nan        nan        nan        nan\n",
        "         nan        nan        nan        nan]\n",
        " [0.87050781 0.97099609        nan        nan        nan        nan\n",
        "         nan        nan        nan        nan]\n",
        " [0.83818359 0.93447266 0.97226563        nan        nan        nan\n",
        "         nan        nan        nan        nan]\n",
        " [0.853125   0.92792969 0.94248047 0.95810547        nan        nan\n",
        "         nan        nan        nan        nan]\n",
        " [0.846875   0.92607422 0.93349609 0.92880859 0.97148437        nan\n",
        "         nan        nan        nan        nan]\n",
        " [0.82705078 0.91806641 0.92568359 0.92324219 0.95507812 0.965625\n",
        "         nan        nan        nan        nan]\n",
        " [0.83408203 0.89345703 0.90566406 0.90849609 0.92958984 0.95117188\n",
        "  0.96357422        nan        nan        nan]\n",
        " [0.80917969 0.88017578 0.89794922 0.89121094 0.92207031 0.93154297\n",
        "  0.94697266 0.95410156        nan        nan]\n",
        " [0.7984375  0.88740234 0.88300781 0.90722656 0.89091797 0.92548828\n",
        "  0.92578125 0.93994141 0.95878906        nan]\n",
        " [0.79658203 0.87373047 0.85712891 0.87724609 0.89970703 0.91044922\n",
        "  0.92109375 0.93837891 0.9453125  0.95722656]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voGzG7X3BD7h"
      },
      "source": [
        "Width 50\n",
        "\n",
        "22:52:17\n",
        "23:03:51\n",
        "100%|██████████| 10/10 [11:34<00:00, 69.44s/trial, best loss: -0.6580235272988505]\n",
        "{'beta': 0.3594520045264994}\n",
        "\n",
        "\n",
        "Width 100\n",
        "\n",
        "23:07:03\n",
        "23:19:06\n",
        "100%|██████████| 10/10 [12:03<00:00, 72.34s/trial, best loss: -0.7445559446839081]\n",
        "{'beta': 0.4821937485554201}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTkCsaCWUF4A"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if params['experiment_name'] == 'random': print(random_split_dataloaders)\n",
        "\n",
        "if params['perform_auto_vcl']:\n",
        "    horizontal_line_y = params['threshold']\n",
        "    categories = list(range(1, len(auto_vcl_accs) + 1))\n",
        "    plt.scatter(categories, auto_vcl_model.likelihoods, marker='x', linestyle='-')\n",
        "    plt.axhline(y=horizontal_line_y, color='r', linestyle='--')\n",
        "\n",
        "if params['perform_vcl']:\n",
        "    plt.xlabel('Dataset number')\n",
        "    plt.ylabel('Best likelihood')\n",
        "\n",
        "    plt.xticks(categories)\n",
        "    plt.ylim(-0.05, 1.05)\n",
        "    plt.show()\n",
        "\n",
        "    auto_vcl_averages = np.nanmean(auto_vcl_accs, axis=1, keepdims=False)\n",
        "\n",
        "    categories = list(range(1, len(vcl_accs) + 1))\n",
        "    print(vcl_accs.shape)\n",
        "    vcl_averages = np.nanmean(vcl_accs, axis=1, keepdims=False)\n",
        "    plt.plot(categories, vcl_averages, label='VCL')\n",
        "    plt.plot(categories, auto_vcl_averages, label='AutoVCL')\n",
        "\n",
        "plt.xlabel('Number of encountered datasets')\n",
        "plt.ylabel('Average test accuracy')\n",
        "plt.xticks(categories)\n",
        "plt.legend()\n",
        "plt.ylim(0, 1.05)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}