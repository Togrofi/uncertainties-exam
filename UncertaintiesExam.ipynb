{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Togrofi/uncertainties-exam/blob/main/UncertaintiesExam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oM3NP03lt0h"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.nn import Parameter\n",
        "from torch.utils.data import Subset, DataLoader, RandomSampler, SubsetRandomSampler\n",
        "from torch.optim import Adam, SGD\n",
        "from tqdm import tqdm\n",
        "from hyperopt import hp, tpe, fmin\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "params = {\n",
        "    'beta': 1,\n",
        "    'proportion_of_dataset': 1,\n",
        "    'threshold': 0.1,\n",
        "    'num_epochs': 10,\n",
        "    'batch_size': 256,\n",
        "    'experiment_name': 'split', # 'split', 'permuted', 'custom'\n",
        "    'perform_baseline': True,\n",
        "    'perform_custom': True,\n",
        "}\n",
        "\n",
        "split_params = {\n",
        "    'hidden_dim': 256\n",
        "}\n",
        "\n",
        "permuted_params = {\n",
        "    'hidden_dim': 100\n",
        "}\n",
        "\n",
        "custom_params = {\n",
        "    'num_clients': 10,\n",
        "}\n",
        "\n",
        "baseline_params = {\n",
        "    'num_tasks': 5,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "a6T3YPtufAir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create Permuted Data\n",
        "def get_permuted_dataloaders(num_tasks, batch_size=256):\n",
        "    dataset = datasets.MNIST\n",
        "    dataloaders = []\n",
        "\n",
        "    for task in range(num_tasks):\n",
        "        if task == 0:\n",
        "            transform = transforms.Compose([\n",
        "                transforms.Resize((28, 28)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.1307,), (0.3081,))\n",
        "            ])\n",
        "        else:\n",
        "            rng_permute = np.random.RandomState(task)\n",
        "            idx_permute = torch.from_numpy(rng_permute.permutation(28*28))\n",
        "            transform = transforms.Compose([\n",
        "                transforms.Resize((28, 28)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.1307,), (0.3081,)),\n",
        "                transforms.Lambda(lambda x, idx=idx_permute: x.view(-1)[idx].view(1, 28, 28))\n",
        "            ])\n",
        "\n",
        "        full_task_trainset = dataset(root=\"./data\", train=True, download=True, transform=transform)\n",
        "        jump_size = int(1 / params['proportion_of_dataset'])\n",
        "        task_trainset = Subset(full_task_trainset, range(0, len(full_task_trainset), jump_size))\n",
        "        task_train_sampler = RandomSampler(task_trainset)\n",
        "        task_train_loader = DataLoader(task_trainset, batch_size=batch_size, sampler=task_train_sampler)\n",
        "\n",
        "        full_task_testset = dataset(root=\"./data\", train=False, download=True, transform=transform)\n",
        "        jump_size = int(1 / params['proportion_of_dataset'])\n",
        "        task_testset = Subset(full_task_testset, range(0, len(full_task_testset), jump_size))\n",
        "        task_test_sampler = RandomSampler(task_testset)\n",
        "        task_test_loader = DataLoader(task_testset, batch_size=batch_size, sampler=task_test_sampler)\n",
        "\n",
        "        dataloaders.append((task_train_loader, task_test_loader))\n",
        "\n",
        "    return dataloaders\n"
      ],
      "metadata": {
        "id": "ynx9u5CqNRGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create Split Data\n",
        "def _extract_class_specific_idx(dataset, target_classes):\n",
        "    idx = torch.zeros_like(dataset.targets, dtype=torch.bool)\n",
        "    for target in target_classes:\n",
        "        idx = idx | (dataset.targets==target)\n",
        "\n",
        "    return idx\n",
        "\n",
        "def get_split_dataloaders(class_distribution, batch_size=256):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((28, 28)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    dataset = datasets.MNIST\n",
        "    trainset = dataset(root=\"./data\", train=True, download=True, transform=transform)\n",
        "    testset = dataset(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "    dataloaders = []\n",
        "\n",
        "    for i, classes in enumerate(class_distribution):\n",
        "        offset = 2 * i\n",
        "        train_idx = _extract_class_specific_idx(trainset, classes)\n",
        "        train_idx = torch.where(train_idx)[0]\n",
        "        sub_train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
        "        sub_train_loader = torch.utils.data.DataLoader(\n",
        "            trainset, batch_size=batch_size, sampler=sub_train_sampler)\n",
        "        trainset.targets[sub_train_sampler.indices] -= offset\n",
        "\n",
        "        test_idx = _extract_class_specific_idx(testset, classes)\n",
        "        test_idx = torch.where(test_idx)[0]\n",
        "        sub_test_sampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
        "        sub_test_loader = torch.utils.data.DataLoader(\n",
        "            testset, batch_size=batch_size, sampler=sub_test_sampler)\n",
        "        testset.targets[sub_test_sampler.indices] -= offset\n",
        "\n",
        "        dataloaders.append((sub_train_loader, sub_test_loader))\n",
        "\n",
        "    return dataloaders\n",
        "\n",
        "def get_split_dataloaders2(class_distribution, num_clients, batch_size=256):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((28, 28)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    dataset = datasets.MNIST\n",
        "    trainset = dataset(root=\"./data\", train=True, download=True, transform=transform)\n",
        "    testset = dataset(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "    dataloaders = []\n",
        "\n",
        "    for i, classes in enumerate(class_distribution):\n",
        "        offset = 2 * i\n",
        "        train_idx = _extract_class_specific_idx(trainset, classes)\n",
        "        train_idx = torch.where(train_idx)[0]\n",
        "        sub_train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
        "        sub_train_loader = torch.utils.data.DataLoader(\n",
        "            trainset, batch_size=batch_size, sampler=sub_train_sampler)\n",
        "        trainset.targets[sub_train_sampler.indices] -= offset\n",
        "\n",
        "        test_idx = _extract_class_specific_idx(testset, classes)\n",
        "        test_idx = torch.where(test_idx)[0]\n",
        "        sub_test_sampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
        "        sub_test_loader = torch.utils.data.DataLoader(\n",
        "            testset, batch_size=batch_size, sampler=sub_test_sampler)\n",
        "        testset.targets[sub_test_sampler.indices] -= offset\n",
        "\n",
        "        dataloaders.append((sub_train_loader, sub_test_loader))\n",
        "\n",
        "    random.shuffle(dataloaders)\n",
        "\n",
        "    return dataloaders\n"
      ],
      "metadata": {
        "id": "eHkWKwgMJyzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Models"
      ],
      "metadata": {
        "id": "LQgAeOrKfF6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Custom Layer\n",
        "def KL_DIV(mu_p, sig_p, mu_q, sig_q):\n",
        "    kl = 0.5 * (2 * torch.log(sig_p / sig_q) - 1 + (sig_q / sig_p).pow(2) + ((mu_p - mu_q) / sig_p).pow(2)).sum()\n",
        "    return kl\n",
        "\n",
        "class BBBLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, bias=True, priors=None):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.use_bias = bias\n",
        "\n",
        "        if priors is None:\n",
        "            priors = {\n",
        "                'prior_mu': 0,\n",
        "                'prior_sigma': 0.01,\n",
        "            }\n",
        "\n",
        "        self.prior_W_mu = torch.tensor(priors['prior_mu'])\n",
        "        self.prior_W_sigma = torch.tensor(priors['prior_sigma'])\n",
        "        self.prior_bias_mu = torch.tensor(priors['prior_mu'])\n",
        "        self.prior_bias_sigma = torch.tensor(priors['prior_sigma'])\n",
        "\n",
        "        self.W_mu = Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.W_rho = Parameter(torch.Tensor(out_features, in_features))\n",
        "        if self.use_bias:\n",
        "            self.bias_mu = Parameter(torch.Tensor(out_features))\n",
        "            self.bias_rho = Parameter(torch.Tensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias_mu', None)\n",
        "            self.register_parameter('bias_rho', None)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.W_mu.data.normal_(0, 0.1)\n",
        "        self.W_rho.data.fill_(-3)\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias_mu.data.normal_(0, 0.1)\n",
        "            self.bias_rho.data.fill_(-3)\n",
        "\n",
        "    def forward(self, x, sample=True):\n",
        "        self.W_sigma = torch.log1p(torch.exp(self.W_rho))\n",
        "        if self.use_bias:\n",
        "            self.bias_sigma = torch.log1p(torch.exp(self.bias_rho))\n",
        "            bias_var = self.bias_sigma ** 2\n",
        "        else:\n",
        "            self.bias_sigma = bias_var = None\n",
        "\n",
        "        act_mu = F.linear(x, self.W_mu, self.bias_mu)\n",
        "        # What is this line?\n",
        "        act_var = 1e-16 + F.linear(x ** 2, self.W_sigma ** 2, bias_var)\n",
        "        act_std = torch.sqrt(act_var)\n",
        "\n",
        "        if self.training or sample:\n",
        "            eps = torch.empty(act_mu.size()).normal_(0, 1).to(act_mu.device)\n",
        "            return act_mu + act_std * eps\n",
        "        else:\n",
        "            return act_mu\n",
        "\n",
        "    def kl_loss(self):\n",
        "        kl = KL_DIV(self.prior_W_mu, self.prior_W_sigma, self.W_mu, self.W_sigma)\n",
        "        if self.use_bias:\n",
        "            kl += KL_DIV(self.prior_bias_mu, self.prior_bias_sigma, self.bias_mu, self.bias_sigma)\n",
        "        return kl"
      ],
      "metadata": {
        "id": "BOQxQIgJdpos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dynamic Head Model\n",
        "\n",
        "class DynamicHeadModel(nn.Module):\n",
        "    def __init__(self, input_size=28*28, hidden_dim=100, hidden_layers=2):\n",
        "        super().__init__()\n",
        "        self.input_layer = BBBLinear(input_size, hidden_dim)\n",
        "        self.shared_layers = nn.ModuleList([BBBLinear(hidden_dim, hidden_dim) for _ in range(hidden_layers)])\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Initially empty, will create new heads dynamically as batches come in.\n",
        "        self.head_layers = nn.ModuleList()\n",
        "        self.num_heads = 0\n",
        "\n",
        "        # Create a lookup dictionary so that each batch index is linked to the\n",
        "        # index of the head that gets used on that batch (unlike in the VCL paper\n",
        "        # there is not necessarily one head per batch).\n",
        "        self.client_id_to_head_id = {}\n",
        "\n",
        "    def forward(self, x, id, use_client_id=False):\n",
        "        head_id = self.client_id_to_head_id[id] if use_client_id else id\n",
        "\n",
        "        x = x.view(-1, 784)\n",
        "        x = F.relu(self.input_layer(x))\n",
        "\n",
        "        for layer in self.shared_layers:\n",
        "            x = F.relu(layer(x))\n",
        "\n",
        "        head_to_use = self.head_layers[head_id]\n",
        "\n",
        "        return head_to_use(x)\n",
        "\n",
        "    def create_new_head(self, client_id, width):\n",
        "        print('\\nCreating a new head!')\n",
        "        new_head_id = len(self.head_layers) # 0 if no head layers yet, 1 if there is 1 etc\n",
        "        self.client_id_to_head_id[client_id] = new_head_id\n",
        "        self.head_layers.append(BBBLinear(self.hidden_dim, width).to(device))\n",
        "        self.num_heads += 1\n",
        "\n",
        "    def get_kl(self, id, use_client_id=False):\n",
        "        head_id = self.client_id_to_head_id[id] if use_client_id else id\n",
        "        kl = 0.0\n",
        "        kl += self.input_layer.kl_loss()\n",
        "        for layer in self.shared_layers:\n",
        "            kl += layer.kl_loss()\n",
        "\n",
        "        kl += self.head_layers[head_id].kl_loss()\n",
        "\n",
        "        return kl\n",
        "\n",
        "    def update_prior(self):\n",
        "        for child in self.children():\n",
        "            if child.__class__ is not nn.ModuleList:\n",
        "                child.prior_W_mu = child.W_mu.data\n",
        "                child.prior_W_sigma = child.W_sigma.data\n",
        "                if child.use_bias:\n",
        "                    child.prior_bias_mu = child.bias_mu.data\n",
        "                    child.prior_bias_sigma = child.bias_sigma.data\n",
        "            if child.__class__ is nn.ModuleList:\n",
        "                for layer in child:\n",
        "                    layer.prior_W_mu = layer.W_mu.data\n",
        "                    layer.prior_W_sigma = layer.W_sigma.data\n",
        "                    if layer.use_bias:\n",
        "                        layer.prior_bias_mu = layer.bias_mu.data\n",
        "                        layer.prior_bias_sigma = layer.bias_sigma.data"
      ],
      "metadata": {
        "id": "X_09osLpRh7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline Models"
      ],
      "metadata": {
        "id": "K6aiCCIUYmk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Baseline Permuted Model\n",
        "class PermutedModel(nn.Module):\n",
        "    def __init__(self, input_size=28*28, hidden_dim=100, output_dim=10):\n",
        "        self.input_size = input_size\n",
        "        super().__init__()\n",
        "        self.fc1 = BBBLinear(input_size, hidden_dim)\n",
        "        self.fc2 = BBBLinear(hidden_dim, hidden_dim)\n",
        "        self.head_layer = BBBLinear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, task_id, use_client_id=False):\n",
        "        out = x.view(-1, self.input_size)\n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = F.relu(self.fc2(out))\n",
        "        return self.head_layer(out)\n",
        "\n",
        "    def get_kl(self, task_id):\n",
        "        kl = 0.0\n",
        "        kl += self.fc1.kl_loss()\n",
        "        kl += self.fc2.kl_loss()\n",
        "        kl += self.head_layer.kl_loss()\n",
        "        return kl\n",
        "\n",
        "    def update_prior(self):\n",
        "        self.fc1.prior_W_mu = self.fc1.W_mu.data\n",
        "        self.fc1.prior_W_sigma = self.fc1.W_sigma.data\n",
        "        if self.fc1.use_bias:\n",
        "            self.fc1.prior_bias_mu = self.fc1.bias_mu.data\n",
        "            self.fc1.prior_bias_sigma = self.fc1.bias_sigma.data\n",
        "\n",
        "        self.fc2.prior_W_mu = self.fc2.W_mu.data\n",
        "        self.fc2.prior_W_sigma = self.fc2.W_sigma.data\n",
        "        if self.fc2.use_bias:\n",
        "            self.fc2.prior_bias_mu = self.fc2.bias_mu.data\n",
        "            self.fc2.prior_bias_sigma = self.fc2.bias_sigma.data\n",
        "\n",
        "        self.head_layer.prior_W_mu = self.head_layer.W_mu.data\n",
        "        self.head_layer.prior_W_sigma = self.head_layer.W_sigma.data\n",
        "        if self.head_layer.use_bias:\n",
        "            self.head_layer.prior_bias_mu = self.head_layer.bias_mu.data\n",
        "            self.head_layer.prior_bias_sigma = self.head_layer.bias_sigma.data"
      ],
      "metadata": {
        "id": "xTK2oDN0YdCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Baseline Split Model\n",
        "class SplitModel(nn.Module):\n",
        "    def __init__(self, input_size=28*28, hidden_dim=100, output_dim=2):\n",
        "        self.input_size = input_size\n",
        "        super().__init__()\n",
        "        self.fc1 = BBBLinear(input_size, hidden_dim)\n",
        "        self.fc2 = BBBLinear(hidden_dim, hidden_dim)\n",
        "        self.head_layers = nn.ModuleList([BBBLinear(hidden_dim, output_dim) for i in range(5)])\n",
        "\n",
        "    def forward(self, x, task_id, use_client_id=False):\n",
        "        out = x.view(-1, self.input_size)\n",
        "        for layer in self.children():\n",
        "            if layer.__class__ is not nn.ModuleList:\n",
        "                out = F.relu(layer(out))\n",
        "        return self.head_layers[task_id](out)\n",
        "\n",
        "    def get_kl(self, task_id):\n",
        "        kl = 0.0\n",
        "        for layer in self.children():\n",
        "            if layer.__class__ is not nn.ModuleList:\n",
        "                kl += layer.kl_loss()\n",
        "        kl += self.head_layers[task_id].kl_loss()\n",
        "        return kl\n",
        "\n",
        "    def update_prior(self):\n",
        "        for layer in self.children():\n",
        "            if layer.__class__ is not nn.ModuleList:\n",
        "                layer.prior_W_mu = layer.W_mu.data\n",
        "                layer.prior_W_sigma = layer.W_sigma.data\n",
        "                if layer.use_bias:\n",
        "                    layer.prior_bias_mu = layer.bias_mu.data\n",
        "                    layer.prior_bias_sigma = layer.bias_sigma.data"
      ],
      "metadata": {
        "id": "_-C0C4H6YiDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "X8IlTCRLfT4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Coreset\n",
        "def attach_random_coreset_permuted(coresets, sub_train_loader, num_samples=200):\n",
        "    \"\"\"\n",
        "    coresets: list of collection of coreset dataloaders\n",
        "    sub_train_loader: loader from which a random coreset is to be drawn\n",
        "    num_samples: number of samples in each coreset\n",
        "    \"\"\"\n",
        "    shuffled_task_indices = torch.randperm(len(sub_train_loader.dataset))\n",
        "    coreset_indices = shuffled_task_indices[:num_samples]\n",
        "    coreset_sampler = SubsetRandomSampler(coreset_indices)\n",
        "    coreset_loader = DataLoader(\n",
        "        sub_train_loader.dataset, batch_size=sub_train_loader.batch_size, sampler=coreset_sampler\n",
        "    )\n",
        "    coresets.append(coreset_loader)\n",
        "\n",
        "def attach_random_coreset_split(coresets, sub_train_loader, num_samples=200):\n",
        "    \"\"\"\n",
        "    coresets: list of collection of coreset dataloaders\n",
        "    sub_train_loader: loader from which a random coreset is to be drawn\n",
        "    num_samples: number of samples in each coreset\n",
        "    \"\"\"\n",
        "    task_indices = sub_train_loader.sampler.indices\n",
        "    shuffled_task_indices = task_indices[torch.randperm(len(task_indices))]\n",
        "    coreset_indices = shuffled_task_indices[:num_samples]\n",
        "    sub_train_loader.sampler.indices = shuffled_task_indices[num_samples:]  # Delete coreset from orginal data\n",
        "    coreset_sampler = torch.utils.data.SubsetRandomSampler(coreset_indices)\n",
        "    coreset_loader = torch.utils.data.DataLoader(\n",
        "        sub_train_loader.dataset, batch_size=sub_train_loader.batch_size, sampler=coreset_sampler)\n",
        "    coresets.append(coreset_loader)\n"
      ],
      "metadata": {
        "id": "_12aDBaPiOUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ELBO\n",
        "class ELBO(nn.Module):\n",
        "    def __init__(self, model, beta):\n",
        "        super().__init__()\n",
        "        self.num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "        self.beta = beta\n",
        "\n",
        "    def forward(self, outputs, targets, kl):\n",
        "        assert not targets.requires_grad\n",
        "        #print(F.nll_loss(outputs, targets, reduction='mean'), self.beta * kl / self.num_params)\n",
        "        # This should only be divided by the number of paramters that were used (in the appropriate head).\n",
        "        return F.nll_loss(outputs, targets, reduction='mean') + self.beta * kl / self.num_params\n",
        "\n",
        "def calculate_accuracy(outputs, targets):\n",
        "    return np.mean(outputs.argmax(dim=-1).cpu().numpy() == targets.cpu().numpy())\n"
      ],
      "metadata": {
        "id": "rDy55WNx61Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get Probability of Head Being the Correct One\n",
        "def get_probability_of_head(model, dataloader, head_id):\n",
        "    inputs  = torch.cat([i for i, _ in dataloader]).to(device)\n",
        "    targets = torch.cat([t.flatten() for _, t in dataloader]).to(device)\n",
        "    output_nodes = torch.max(targets).item() + 1\n",
        "    log_output = monte_carlo(model, inputs, output_nodes, head_id)\n",
        "    loss = F.nll_loss(log_output, targets, reduction='mean')\n",
        "    probability = torch.exp(-loss)\n",
        "    return probability"
      ],
      "metadata": {
        "id": "thJ8BgM2EMH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Monte Carlo\n",
        "def monte_carlo(model, inputs, output_nodes, id, use_client_id=False, no_grad=False):\n",
        "    T = 10\n",
        "    outputs = torch.zeros(inputs.shape[0], output_nodes, T).to(device)\n",
        "\n",
        "    for i in range(T):\n",
        "        if no_grad:\n",
        "            with torch.no_grad():\n",
        "                out = model(inputs, id, use_client_id)\n",
        "        else:\n",
        "            out = model(inputs, id, use_client_id)\n",
        "        outputs[:, :, i] = F.log_softmax(out, dim=-1)\n",
        "\n",
        "    log_output = torch.logsumexp(outputs, dim=-1) - np.log(T)\n",
        "\n",
        "    return log_output\n"
      ],
      "metadata": {
        "id": "VWCYS5W1kcM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VCL Methods"
      ],
      "metadata": {
        "id": "cILWlU6VYz96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dynamic Train and Predict\n",
        "def dynamic_train(model, num_epochs, dataloader, client_id, beta, replay=False):\n",
        "    beta = 0 if replay else beta\n",
        "    lr_start = 1e-3\n",
        "\n",
        "    flattened_tensor = torch.cat([t for _, t in dataloader])\n",
        "    output_nodes = torch.max(flattened_tensor).item() + 1\n",
        "\n",
        "    elbo = ELBO(model, beta)\n",
        "    optimizer = Adam(model.parameters(), lr=lr_start)\n",
        "\n",
        "    head_loss = None\n",
        "\n",
        "    model.train()\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "    #for epoch in range(num_epochs):\n",
        "        for inputs, targets in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            # Check if we have identified this client_id with a certain head already:\n",
        "            if client_id not in model.client_id_to_head_id.keys():\n",
        "                # Otherwise, we need to find which head to use.\n",
        "                # First, calculate the loss for each existing head:\n",
        "                most_promising_head_id   = None\n",
        "                most_promising_head_prob = 0\n",
        "                for head_id, head in enumerate(model.head_layers):\n",
        "                    if head.out_features == output_nodes:\n",
        "                        prob_of_this_head = get_probability_of_head(model, dataloader, head_id)\n",
        "                        if prob_of_this_head > most_promising_head_prob:\n",
        "                            most_promising_head_id   = head_id\n",
        "                            most_promising_head_prob = prob_of_this_head\n",
        "\n",
        "                print(most_promising_head_prob)\n",
        "\n",
        "                # If the results are too poor, create a new head for it\n",
        "                # and calculate the loss on this new head.\n",
        "                if most_promising_head_prob <= params['threshold']:\n",
        "                    # We couldn't find a strong head, so make a new one.\n",
        "                    model.create_new_head(client_id, output_nodes)\n",
        "                else:\n",
        "                    model.client_id_to_head_id[client_id] = most_promising_head_id\n",
        "\n",
        "            log_output = monte_carlo(model, inputs, output_nodes, client_id, use_client_id=True)\n",
        "            kl = model.get_kl(client_id, use_client_id=True)\n",
        "            head_loss = elbo(log_output, targets, kl)\n",
        "            head_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "def dynamic_predict(model, dataloader, client_id):\n",
        "    model.eval()\n",
        "    accs = []\n",
        "\n",
        "    flattened_tensor = torch.cat([t.flatten() for _, t in dataloader])\n",
        "    output_nodes = torch.max(flattened_tensor).item() + 1\n",
        "\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        #outputs = torch.zeros(inputs.shape[0], output_nodes, model.num_heads).to(device)\n",
        "#\n",
        "        #for i in range(model.num_heads):\n",
        "        #    with torch.no_grad():\n",
        "        #        out = model(inputs, client_id, use_client_id=True)\n",
        "        #    outputs[:, :, i] = F.log_softmax(out, dim=-1)\n",
        "#\n",
        "        #log_output = torch.logsumexp(outputs, dim=-1) - np.log(model.num_heads)\n",
        "        log_output = monte_carlo(model, inputs, output_nodes, client_id, use_client_id=True, no_grad=True)\n",
        "        accs.append(calculate_accuracy(log_output, targets))\n",
        "\n",
        "    return np.mean(accs)\n"
      ],
      "metadata": {
        "id": "BhNa39IfixRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dynamic VCL\n",
        "def dynamic_vcl(num_epochs, dataloaders, model, coreset_method, coreset_size=0, beta=1., update_prior=True):\n",
        "    num_batches = len(dataloaders)\n",
        "    coreset_list = []\n",
        "    all_accs = np.empty(shape=(num_batches, num_batches))\n",
        "    all_accs.fill(np.nan)\n",
        "\n",
        "    for client_id, (trainloader, testloader) in enumerate(dataloaders):\n",
        "        # Train on non-coreset data\n",
        "        dynamic_train(model, num_epochs, trainloader, client_id, beta)\n",
        "        print(\"Done Training Task\", client_id + 1)\n",
        "\n",
        "        # Attach a new coreset\n",
        "        if coreset_size > 0:\n",
        "            coreset_method(coreset_list, trainloader, num_samples=coreset_size)\n",
        "\n",
        "            # Replay old tasks using coresets\n",
        "            for task in range(client_id + 1):\n",
        "                print(\"Replaying Task\", task + 1)\n",
        "                dynamic_train(model, num_epochs, coreset_list[task], task, beta, replay=True)\n",
        "        print()\n",
        "\n",
        "        # Evaluate on old tasks\n",
        "        for client_to_retest_id in range(client_id + 1):\n",
        "            _, testloader_i = dataloaders[client_to_retest_id]\n",
        "            accuracy = dynamic_predict(model, testloader_i, client_to_retest_id)\n",
        "            print(\"Task {} Accuracy: {}\".format(client_to_retest_id + 1, accuracy))\n",
        "            all_accs[client_id][client_to_retest_id] = accuracy\n",
        "        print()\n",
        "\n",
        "        if update_prior:\n",
        "            model.update_prior()\n",
        "    print(all_accs)\n",
        "    return all_accs"
      ],
      "metadata": {
        "id": "Sy3T_IQFVSzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Baseline Train and Predict\n",
        "def baseline_train(model, num_epochs, dataloader, task_id, replay=False):\n",
        "    beta = 0 if replay else 1\n",
        "    lr_start = 1e-3\n",
        "\n",
        "    flattened_tensor = torch.cat([t.flatten() for _, t in dataloader])\n",
        "    output_nodes = torch.max(flattened_tensor).item() + 1\n",
        "\n",
        "    elbo = ELBO(model, beta)\n",
        "    optimizer = Adam(model.parameters(), lr=lr_start)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "        for inputs, targets in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            #outputs = torch.zeros(inputs.shape[0], output_nodes, T, device=device)\n",
        "#\n",
        "            #for i in range(T):\n",
        "            #    net_out = model(inputs, task_id)\n",
        "            #    outputs[:, :, i] = F.log_softmax(net_out, dim=-1)\n",
        "#\n",
        "            #log_output = torch.logsumexp(outputs, dim=-1) - np.log(T)\n",
        "            log_output = monte_carlo(model, inputs, output_nodes, task_id)\n",
        "\n",
        "            kl = model.get_kl(task_id)\n",
        "            loss = elbo(log_output, targets, kl)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "def baseline_predict(model, dataloader, task_id):\n",
        "    flattened_tensor = torch.cat([t.flatten() for _, t in dataloader])\n",
        "    output_nodes = torch.max(flattened_tensor).item() + 1\n",
        "\n",
        "    model.train()\n",
        "    accs = []\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        #outputs = torch.zeros(inputs.shape[0], output_nodes, T, device=device)\n",
        "\n",
        "        #for i in range(T):\n",
        "        #    with torch.no_grad():\n",
        "        #        net_out = model(inputs, task_id)\n",
        "        #    outputs[:, :, i] = F.log_softmax(net_out, dim=-1)\n",
        "\n",
        "        #log_output = torch.logsumexp(outputs, dim=-1) - np.log(T)\n",
        "        log_output = monte_carlo(model, inputs, output_nodes, task_id, no_grad=True)\n",
        "        accs.append(calculate_accuracy(log_output, targets))\n",
        "\n",
        "    return np.mean(accs)\n"
      ],
      "metadata": {
        "id": "ahYdBEamfdc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Baseline VCL\n",
        "def baseline_vcl(num_tasks, num_epochs, dataloaders, model, coreset_method, coreset_size=0, update_prior=True):\n",
        "    coreset_list = []\n",
        "    all_accs = np.empty(shape=(num_tasks, num_tasks))\n",
        "    all_accs.fill(np.nan)\n",
        "    for task_id in range(num_tasks):\n",
        "        print(\"Starting Task\", task_id + 1)\n",
        "\n",
        "        # Train on non-coreset data\n",
        "        trainloader, testloader = dataloaders[task_id]\n",
        "        baseline_train(model, num_epochs, trainloader, task_id)\n",
        "        print(\"Done Training Task\", task_id + 1)\n",
        "\n",
        "        # Attach a new coreset\n",
        "        if coreset_size > 0:\n",
        "            coreset_method(coreset_list, trainloader, num_samples=coreset_size)\n",
        "\n",
        "            # Replay old tasks using coresets\n",
        "            for task in range(task_id + 1):\n",
        "                print(\"Replaying Task\", task + 1)\n",
        "                baseline_train(model, num_epochs, coreset_list[task], task_id=task, replay=True)\n",
        "        print()\n",
        "\n",
        "        # Evaluate on old tasks\n",
        "        for task in range(task_id + 1):\n",
        "            _, testloader_i = dataloaders[task]\n",
        "            accuracy = baseline_predict(model, testloader_i, task)\n",
        "            print(\"Task {} Accuracy: {}\".format(task + 1, accuracy))\n",
        "            all_accs[task_id][task] = accuracy\n",
        "        print()\n",
        "        if update_prior:\n",
        "            model.update_prior()\n",
        "    print(all_accs)\n",
        "    return all_accs"
      ],
      "metadata": {
        "id": "t9uWtW39YTc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments"
      ],
      "metadata": {
        "id": "e4gBsipVfuC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run Experiments\n",
        "num_tasks = 5\n",
        "\n",
        "permuted_dataloaders = get_permuted_dataloaders(num_tasks)\n",
        "split_class_distribution = [\n",
        "    [0, 1],\n",
        "    [2, 3],\n",
        "    [4, 5],\n",
        "    [6, 7],\n",
        "    [8, 9]]\n",
        "split_dataloaders = get_split_dataloaders(split_class_distribution)\n",
        "\n",
        "coreset_method = attach_random_coreset_split\n",
        "if params['perform_baseline']:\n",
        "    baseline_model = SplitModel(hidden_dim=256).to(device)\n",
        "    baseline_accs = baseline_vcl(num_tasks, params['num_epochs'], dataloaders, baseline_model, coreset_method, 0)[-1]\n",
        "if params['perform_custom']:\n",
        "    dynamic_model = DynamicHeadModel(hidden_dim=256).to(device)\n",
        "    dynamic_accs = dynamic_vcl(params['num_epochs'], dataloaders, dynamic_model, coreset_method, 0, params['beta'])[-1]\n",
        "\n",
        "#space = {\n",
        "#    'beta': hp.uniform('beta', 0, 1),  # Uniform distribution between -10 and 10 for parameter y\n",
        "#    'lmda': hp.uniform('lmda', 1, 10000)   # Uniform distribution between -10 and 10 for parameter z\n",
        "#}\n",
        "#best = fmin(fn=evaluate_hyperparams,\n",
        "#            space=space,\n",
        "#            algo=tpe.suggest,\n",
        "#            max_evals=2)\n",
        "\n",
        "#print(best)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sBF5xv2DIpw",
        "outputId": "462f913b-4f24-4f97-a4d3-b0e7d6d302e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "11:05:15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Creating a new head!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:49<00:00,  4.96s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done Training Task 1\n",
            "Replaying Task 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 23.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Task 1 Accuracy: 1.0\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0047, device='cuda:0', grad_fn=<ExpBackward0>)\n",
            "Creating a new head!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:50<00:00,  5.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done Training Task 2\n",
            "Replaying Task 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 22.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replaying Task 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 21.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Task 1 Accuracy: 0.9995659722222222\n",
            "Task 2 Accuracy: 0.98874609375\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0072, device='cuda:0', grad_fn=<ExpBackward0>)\n",
            "Creating a new head!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:50<00:00,  5.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done Training Task 3\n",
            "Replaying Task 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 23.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replaying Task 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 22.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replaying Task 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 22.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Task 1 Accuracy: 0.9970395418739635\n",
            "Task 2 Accuracy: 0.9264765625\n",
            "Task 3 Accuracy: 0.9951171875\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0066, device='cuda:0', grad_fn=<ExpBackward0>)\n",
            "Creating a new head!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:59<00:00,  5.97s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done Training Task 4\n",
            "Replaying Task 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 23.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replaying Task 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 22.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replaying Task 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 22.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replaying Task 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 22.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Task 1 Accuracy: 0.9961714863184079\n",
            "Task 2 Accuracy: 0.9505078124999999\n",
            "Task 3 Accuracy: 0.9755859375\n",
            "Task 4 Accuracy: 0.9788679929123711\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0148, device='cuda:0', grad_fn=<ExpBackward0>)\n",
            "Creating a new head!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [01:01<00:00,  6.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done Training Task 5\n",
            "Replaying Task 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 22.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replaying Task 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 21.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replaying Task 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 19.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replaying Task 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 15.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replaying Task 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 15.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Task 1 Accuracy: 0.9896610696517413\n",
            "Task 2 Accuracy: 0.95840234375\n",
            "Task 3 Accuracy: 0.99267578125\n",
            "Task 4 Accuracy: 0.9800207393685567\n",
            "Task 5 Accuracy: 0.9775288367146597\n",
            "\n",
            "[[1.                nan        nan        nan        nan]\n",
            " [0.99956597 0.98874609        nan        nan        nan]\n",
            " [0.99703954 0.92647656 0.99511719        nan        nan]\n",
            " [0.99617149 0.95050781 0.97558594 0.97886799        nan]\n",
            " [0.98966107 0.95840234 0.99267578 0.98002074 0.97752884]]\n",
            "11:10:14\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.9796577541469915"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs)"
      ],
      "metadata": {
        "id": "6Y6oHPaXL7IA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "4e66aa7b-5618-4713-ddc5-41ca1e5c60de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'outputs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-195-42c520611ffc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Width 50\n",
        "\n",
        "22:52:17\n",
        "23:03:51\n",
        "100%|██████████| 10/10 [11:34<00:00, 69.44s/trial, best loss: -0.6580235272988505]\n",
        "{'beta': 0.3594520045264994}\n",
        "\n",
        "\n",
        "Width 100\n",
        "\n",
        "23:07:03\n",
        "23:19:06\n",
        "100%|██████████| 10/10 [12:03<00:00, 72.34s/trial, best loss: -0.7445559446839081]\n",
        "{'beta': 0.4821937485554201}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "voGzG7X3BD7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualise data\n",
        "f, axarr = plt.subplots(2,2)\n",
        "x_train = next(iter(dataloaders[0][0]))[0]\n",
        "x_train2 = next(iter(dataloaders[1][0]))[0]\n",
        "x_train3 = next(iter(dataloaders[2][0]))[0]\n",
        "#x_train4 = next(iter(dataloaders[3][0]))[0]\n",
        "#x_train5 = next(iter(dataloaders[4][0]))[0]\n",
        "\n",
        "axarr[0,0].imshow(x_train[0, 0], cmap=\"gray\")\n",
        "axarr[0,1].imshow(x_train2[0, 0], cmap=\"gray\")\n",
        "axarr[1,0].imshow(x_train3[0, 0], cmap=\"gray\")\n",
        "#axarr[1,1].imshow(x_train4[0, 0], cmap=\"gray\")\n",
        "np.vectorize(lambda ax:ax.axis('off'))(axarr);"
      ],
      "metadata": {
        "id": "srsjElTVCdHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data\n",
        "categories = ['1', '2', '3', '4', '5']\n",
        "best_confidences = [0, 0.0047, 0.0072, 0.0066, 0.0148]\n",
        "horizontal_line_y = 0.05\n",
        "# Plot\n",
        "plt.scatter(categories, best_confidences, marker='x', linestyle='-')\n",
        "plt.axhline(y=horizontal_line_y, color='r', linestyle='--')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Task number')\n",
        "plt.ylabel('Best likelihood')\n",
        "\n",
        "# Set x-axis tick labels\n",
        "plt.xticks(categories)\n",
        "# Set y-axis limits\n",
        "plt.ylim(0, 1)\n",
        "# Show plot\n",
        "#kplt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CTkCsaCWUF4A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "outputId": "5b0a658d-92dc-4c27-9470-48e959a9df0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}